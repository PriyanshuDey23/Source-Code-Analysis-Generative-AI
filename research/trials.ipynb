{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A over the Code Base to Understand How it Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anything the person is not understanding about the project, he is gonna use that\n",
    "- user will upload one github repro url\n",
    "- from url , clone the repro\n",
    "- Extract the information\n",
    "- rag operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo # Clone any repro from github\n",
    "from langchain.text_splitter import Language # With the help of this i will set python programming\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationSummaryMemory # Creating memory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Source-Code-Analysis-Generative-AI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file test_repro already exists.\n"
     ]
    }
   ],
   "source": [
    "# Make Directory\n",
    "!mkdir test_repro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repro in test_repro\n",
    "repo_path=\"test_repro/\"\n",
    "repo=Repo.clone_from(\"https://github.com/PriyanshuDey23/Medical-ChatBot-using-GenAi\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the codes from the repository(test_repro)\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"], # Python\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repro\\\\ test.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask , render_template ,jsonify ,request\\nfrom src.helper import google_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\\nfrom dotenv import load_dotenv\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom src.prompt import *\\nimport os\\n\\n\\napp = Flask(__name__)\\n\\n# Set Environment variable\\n\\nload_dotenv()\\n# Load the api\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n\\n# Load The Google Api key\\nGOOGLE_API_KEY = os.environ.get(\\'GOOGLE_API_KEY\\')\\n\\n# Embedding\\nembeddings=google_embeddings()\\n\\n# Load Existing index from Database\\n\\nfrom langchain_pinecone import PineconeVectorStore\\n\\nindex_name = \"medicalbot\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\n\\n# Retriver \\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3}) # For showing 3 results\\n\\n# Initialize the LLm\\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-002\",temperature=0.3, max_tokens=500)\\n\\n# Prompt from src\\\\prompt.py\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n# Default Route (interface)\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n# Chat Operation\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"] # Message\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg}) # Give to rag chain\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"]) # Print\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repro\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Priyanshu Dey',\\n    author_email= 'priyanshudey.ds@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repro\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Store index process\\n\\nfrom src.helper import load_pdf_file,text_split,google_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Set Environment variable\\n\\nload_dotenv()\\n# Load the api\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n\\n# Load The Google Api key\\nGOOGLE_API_KEY = os.environ.get(\\'GOOGLE_API_KEY\\')\\n\\n# Extract data\\nextracted_data=load_pdf_file(data=\\'Data/\\')\\n\\n# Create Chunks\\ntext_chunks=text_split(extracted_data)\\n\\n# Embedding\\nembeddings=google_embeddings()\\n\\n# Pinecone Initialization\\n\\n# We will create and push the embedding with the help of python code\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=768, \\n    metric=\"cosine\", \\n    spec=ServerlessSpec(\\n        cloud=\"aws\", \\n        region=\"us-east-1\"\\n    ) \\n) \\n\\n# Embed each chunk and insert the embeddings into your Pinecone index.\\n\\ndocsearch= PineconeVectorStore.from_documents(text_chunks,embeddings,index_name=index_name)'),\n",
       " Document(metadata={'source': 'test_repro\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\", # Python interpreter will consider it as a local environment, which means we can import something from the folder\\n    \"src/helper.py\", # We will write all the functionality\\n    \"src/prompt.py\", # We will write the prompt\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n    \"test.py\"\\n]\\n\\n\\n\\nfor filepath in list_of_files:\\n    filepath= Path(filepath) # define the path type, it will ignore the(/) and create the files and folder\\n    filedir,filename=os.path.split(filepath)# For Separating the file and Folder name\\n\\n    # Create the directory\\n    if filedir != \"\":       # Not  empty,present(if folder is present)\\n        os.makedirs(filedir,exist_ok=True) # Make directory, exist_ok=True:- If it is created then it won\\'t be created again\\n        logging.info(f\" Creating Directory ; {filedir} for the file: {filename}\")\\n\\n    # Creating the files\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath)==0): # Check  file not exists or check the size of the file,if it is empty ,if it is not empty then it will not replace it\\n\\n        with open(filepath, \"w\" ) as f: # Create that file\\n            pass\\n            logging.info(f\"Creating Empty File: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists \")\\n\\n# All Files And Folder will be created'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\\n\\n# Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",  # Load Only Pdf documents\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n# Chunking Operation\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n# Embedding\\ndef google_embeddings():\\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\n\\n\\n\\n\\n\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer in bullet format\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load every code from python in document format\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask , render_template ,jsonify ,request\\nfrom src.helper import google_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\\nfrom dotenv import load_dotenv\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom src.prompt import *\\nimport os\\n\\n\\napp = Flask(__name__)\\n\\n# Set Environment variable\\n\\nload_dotenv()\\n# Load the api\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n\\n# Load The Google Api key\\nGOOGLE_API_KEY = os.environ.get(\\'GOOGLE_API_KEY\\')\\n\\n# Embedding\\nembeddings=google_embeddings()\\n\\n# Load Existing index from Database\\n\\nfrom langchain_pinecone import PineconeVectorStore\\n\\nindex_name = \"medicalbot\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\n\\n# Retriver \\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3}) # For showing 3 results\\n\\n# Initialize the LLm\\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-002\",temperature=0.3, max_tokens=500)\\n\\n# Prompt from src\\\\prompt.py\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n# Default Route (interface)\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n# Chat Operation\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"] # Message\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg}) # Give to rag chain\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"]) # Print\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Aware Splitting\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask , render_template ,jsonify ,request\\nfrom src.helper import google_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\\nfrom dotenv import load_dotenv\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain'),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_core.prompts import ChatPromptTemplate\\nfrom src.prompt import *\\nimport os'),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='app = Flask(__name__)\\n\\n# Set Environment variable\\n\\nload_dotenv()\\n# Load the api\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n\\n# Load The Google Api key\\nGOOGLE_API_KEY = os.environ.get(\\'GOOGLE_API_KEY\\')\\n\\n# Embedding\\nembeddings=google_embeddings()\\n\\n# Load Existing index from Database\\n\\nfrom langchain_pinecone import PineconeVectorStore\\n\\nindex_name = \"medicalbot\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)'),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Retriver \\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3}) # For showing 3 results\\n\\n# Initialize the LLm\\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-002\",temperature=0.3, max_tokens=500)\\n\\n# Prompt from src\\\\prompt.py\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)'),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='question_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n# Default Route (interface)\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repro\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n# Chat Operation\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"] # Message\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg}) # Give to rag chain\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"]) # Print\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repro\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Priyanshu Dey',\\n    author_email= 'priyanshudey.ds@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repro\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"# Store index process\\n\\nfrom src.helper import load_pdf_file,text_split,google_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# Set Environment variable\\n\\nload_dotenv()\\n# Load the api\\nPINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\\n\\n# Load The Google Api key\\nGOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\\n\\n# Extract data\\nextracted_data=load_pdf_file(data='Data/')\"),\n",
       " Document(metadata={'source': 'test_repro\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Create Chunks\\ntext_chunks=text_split(extracted_data)\\n\\n# Embedding\\nembeddings=google_embeddings()\\n\\n# Pinecone Initialization\\n\\n# We will create and push the embedding with the help of python code\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=768, \\n    metric=\"cosine\", \\n    spec=ServerlessSpec(\\n        cloud=\"aws\", \\n        region=\"us-east-1\"\\n    ) \\n)'),\n",
       " Document(metadata={'source': 'test_repro\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Embed each chunk and insert the embeddings into your Pinecone index.\\n\\ndocsearch= PineconeVectorStore.from_documents(text_chunks,embeddings,index_name=index_name)'),\n",
       " Document(metadata={'source': 'test_repro\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\", # Python interpreter will consider it as a local environment, which means we can import something from the folder\\n    \"src/helper.py\", # We will write all the functionality\\n    \"src/prompt.py\", # We will write the prompt\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n    \"test.py\"\\n]'),\n",
       " Document(metadata={'source': 'test_repro\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='for filepath in list_of_files:\\n    filepath= Path(filepath) # define the path type, it will ignore the(/) and create the files and folder\\n    filedir,filename=os.path.split(filepath)# For Separating the file and Folder name'),\n",
       " Document(metadata={'source': 'test_repro\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Create the directory\\n    if filedir != \"\":       # Not  empty,present(if folder is present)\\n        os.makedirs(filedir,exist_ok=True) # Make directory, exist_ok=True:- If it is created then it won\\'t be created again\\n        logging.info(f\" Creating Directory ; {filedir} for the file: {filename}\")'),\n",
       " Document(metadata={'source': 'test_repro\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Creating the files\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath)==0): # Check  file not exists or check the size of the file,if it is empty ,if it is not empty then it will not replace it\\n\\n        with open(filepath, \"w\" ) as f: # Create that file\\n            pass\\n            logging.info(f\"Creating Empty File: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists \")\\n\\n# All Files And Folder will be created'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # Embedding Model\\n\\n# Extract Data From the PDF File'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",  # Load Only Pdf documents\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n# Chunking Operation\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n# Embedding'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def google_embeddings():\\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repro\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer in bullet format\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')\n",
    "# Db is created in research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3636\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-002\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is create_index funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `create_index` function creates a Pinecone index named \"medicalbot\" with a dimension of 768, using the cosine similarity metric.  It is configured as a serverless index on AWS in the us-east-1 region.  It does not appear to be explicitly defined as a Python function, but rather a method call on the Pinecone client object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_pdf_file` function loads all PDF files from a specified directory. It uses the `DirectoryLoader` with `PyPDFLoader` to achieve this, returning a list of `Document` objects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"what is load_pdf_file funtion?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
